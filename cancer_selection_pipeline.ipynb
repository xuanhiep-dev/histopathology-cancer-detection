{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-08-29T21:28:49.155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 0. Libs & seed ==========================================================\n",
    "import math, random, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 1. Data: CIFAR-100 -> resize 227, augmentation mạnh =====================\n",
    "BATCH_SIZE = 512\n",
    "IMG_SIZE = 227\n",
    "\n",
    "# CIFAR-100 stats\n",
    "MEAN = (0.5071, 0.4867, 0.4408)\n",
    "STD  = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandAugment(num_ops=2, magnitude=9),   # N=2 phép, M=9 cường độ\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "transform_test = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "testset  = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "testloader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 2. Model theo đúng mô tả trong paper ====================================\n",
    "class ProposedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    7 conv layers: 4 Conv2d + 1 GroupedConv + 2 TransposedConv\n",
    "    Sau đó: Pool(3x3,s2) -> FC(4096)->Drop(0.5)->FC(4096)->Drop(0.5)->FC(100)\n",
    "    Dùng LazyLinear để không phải đoán flatten_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 13x13, s=6, pad≈\"same\"\n",
    "            nn.Conv2d(3, 128, kernel_size=13, stride=6, padding=3),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2, padding=2),   # \"same\" cho pool\n",
    "\n",
    "            # Conv2: 7x7, s=2, pad=\"same\"\n",
    "            nn.Conv2d(128, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2, padding=2),\n",
    "\n",
    "            # Conv3: 5x5, s=2, pad=\"same\"\n",
    "            nn.Conv2d(64, 48, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Conv4: 3x3, s=2, pad=\"same\"\n",
    "            nn.Conv2d(48, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Grouped Conv5: 3x3, s=1, pad=\"same\", groups=2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Conv6 (Transposed): 3x3, s=1, pad=1, out=256\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Conv7 (Transposed): 3x3, s=1, pad=1, out=128 + BN + Pool(3,s=2)\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)   # \"same\" cho pool\n",
    "        )\n",
    "\n",
    "        # Classifier theo paper: 4096 -> 4096 -> 100, dropout 0.5 giữa các FC\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(4096),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.classifier(x)   # logits\n",
    "        return x\n",
    "\n",
    "net = ProposedCNN(num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# === EarlyStopping helper =====================================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"  No improvement for {self.counter}/{self.patience} epochs\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "\n",
    "# === 3. Loss/Optim/Scheduler ==========================================\n",
    "MIX_ALPHA = 1.0  # Beta distribution alpha cho MixUp/CutMix\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "num_epochs = 500\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=12, delta=0.0)\n",
    "\n",
    "# === MixUp & CutMix ===================================================\n",
    "def mixup_data(x, y, alpha=MIX_ALPHA):\n",
    "    lam = torch.distributions.Beta(alpha, alpha).sample().item() if alpha > 0 else 1.0\n",
    "    index = torch.randperm(x.size(0), device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_data(x, y, alpha=MIX_ALPHA):\n",
    "    lam = torch.distributions.Beta(alpha, alpha).sample().item() if alpha > 0 else 1.0\n",
    "    index = torch.randperm(x.size(0), device=x.device)\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "\n",
    "    # điều chỉnh lại lambda dựa trên diện tích patch\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = (1. - lam) ** 0.5\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform center\n",
    "    cx = random.randint(0, W)\n",
    "    cy = random.randint(0, H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def mix_criterion(pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    with torch.no_grad():\n",
    "        _, p = pred.max(1)\n",
    "        return (p == target).float().mean().item() * 100.0\n",
    "\n",
    "# === 4. Train/Eval loop ======================================================\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss, run_correct, run_total = 0.0, 0, 0\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for i, (inputs, labels) in pbar:\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # chọn ngẫu nhiên MixUp hoặc CutMix\n",
    "        if random.random() < 0.5:\n",
    "            inputs_mixed, y_a, y_b, lam = mixup_data(inputs, labels)\n",
    "        else:\n",
    "            inputs_mixed, y_a, y_b, lam = cutmix_data(inputs, labels)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            outputs = net(inputs_mixed)\n",
    "            loss = mix_criterion(outputs, y_a, y_b, lam)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # ước lượng acc\n",
    "        acc_a = accuracy(outputs, y_a)\n",
    "        acc_b = accuracy(outputs, y_b)\n",
    "        batch_acc = lam * acc_a + (1 - lam) * acc_b\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        run_correct += batch_acc * inputs.size(0)\n",
    "        run_total   += inputs.size(0)\n",
    "\n",
    "        if (i % 50 == 0) or (i == len(trainloader)-1):\n",
    "            avg_loss = running_loss / (i+1)\n",
    "            avg_acc  = run_correct / run_total\n",
    "            pbar.set_postfix({\"Loss\": f\"{avg_loss:.3f}\", \"Train Acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    # ---- Evaluation ----\n",
    "    net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad(), autocast(enabled=True):\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = net(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    test_acc = 100.0 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} finished - Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- Early stopping ----\n",
    "    early_stopping(test_acc, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered! Loading best model weights...\")\n",
    "        net.load_state_dict(early_stopping.best_model_wts)\n",
    "        break\n",
    "\n",
    "print(\"Finished Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(early_stopping.best_model_wts)\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad(), autocast(enabled=True):\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = net(inputs)\n",
    "        _, pred = outputs.max(1)\n",
    "        total   += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "test_acc = 100.0 * correct / total\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} finished - Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"proposed_cnn_cifar100_state.pth\")\n",
    "model = ProposedCNN(num_classes=100).to(device)\n",
    "model.load_state_dict(torch.load(\"proposed_cnn_cifar100_state.pth\"))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = outputs.max(1)\n",
    "        total   += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "test_acc = 100.0 * correct / total\n",
    "print(f\"Epoch {epoch+1}/{num_epochs} finished - Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:01.932362Z",
     "iopub.status.busy": "2025-08-29T21:36:01.931840Z",
     "iopub.status.idle": "2025-08-29T21:36:09.685351Z",
     "shell.execute_reply": "2025-08-29T21:36:09.684752Z",
     "shell.execute_reply.started": "2025-08-29T21:36:01.932332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 0. Libs & seed ==========================================================\n",
    "import math, random, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 2. Model theo đúng mô tả trong paper ====================================\n",
    "class ProposedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    7 conv layers: 4 Conv2d + 1 GroupedConv + 2 TransposedConv\n",
    "    Sau đó: Pool(3x3,s2) -> FC(4096)->Drop(0.5)->FC(4096)->Drop(0.5)->FC(100)\n",
    "    Dùng LazyLinear để không phải đoán flatten_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1: 13x13, s=6, pad≈\"same\"\n",
    "            nn.Conv2d(3, 128, kernel_size=13, stride=6, padding=3),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2, padding=2),   # \"same\" cho pool\n",
    "\n",
    "            # Conv2: 7x7, s=2, pad=\"same\"\n",
    "            nn.Conv2d(128, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2, padding=2),\n",
    "\n",
    "            # Conv3: 5x5, s=2, pad=\"same\"\n",
    "            nn.Conv2d(64, 48, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Conv4: 3x3, s=2, pad=\"same\"\n",
    "            nn.Conv2d(48, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            # Grouped Conv5: 3x3, s=1, pad=\"same\", groups=2\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Conv6 (Transposed): 3x3, s=1, pad=1, out=256\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Conv7 (Transposed): 3x3, s=1, pad=1, out=128 + BN + Pool(3,s=2)\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)   # \"same\" cho pool\n",
    "        )\n",
    "\n",
    "        # Classifier theo paper: 4096 -> 4096 -> 100, dropout 0.5 giữa các FC\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(4096),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.classifier(x)   # logits\n",
    "        return x\n",
    "\n",
    "net = ProposedCNN(num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:09.686859Z",
     "iopub.status.busy": "2025-08-29T21:36:09.686518Z",
     "iopub.status.idle": "2025-08-29T21:36:10.382217Z",
     "shell.execute_reply": "2025-08-29T21:36:10.381444Z",
     "shell.execute_reply.started": "2025-08-29T21:36:09.686839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, torch, torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==== Tải model đã pretrain trên CIFAR-100 ====\n",
    "# Cách 1 (khuyến nghị): state_dict\n",
    "net = ProposedCNN(num_classes=100).to(device)\n",
    "state = torch.load(\"/kaggle/input/proposed_cnn_cifar100/pytorch/default/1/proposed_cnn_cifar100_state.pth\", map_location=device)\n",
    "net.load_state_dict(state)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# ==== Hook để lấy đặc trưng FC2 theo paper ====\n",
    "# Kiến trúc classifier: [Flatten, FC1(4096), LeakyReLU, Dropout,\n",
    "#                        FC2(4096), LeakyReLU, Dropout, FC3(num_classes)]\n",
    "# Ta muốn lấy output sau LeakyReLU của FC2 -> classifier[5]\n",
    "fc2_feats = []\n",
    "def hook_fc2(module, inp, out):\n",
    "    # out shape: (B, 4096)\n",
    "    fc2_feats.append(out.detach().float().cpu())\n",
    "\n",
    "# Đăng ký hook tại classifier[5] (LeakyReLU sau FC2)\n",
    "hook_handle = net.classifier[5].register_forward_hook(hook_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:10.383235Z",
     "iopub.status.busy": "2025-08-29T21:36:10.383006Z",
     "iopub.status.idle": "2025-08-29T21:36:10.391932Z",
     "shell.execute_reply": "2025-08-29T21:36:10.391242Z",
     "shell.execute_reply.started": "2025-08-29T21:36:10.383219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "def load_all_images_ratio(base_path, total=20000, seed=1337):\n",
    "    image_paths_0, image_paths_1 = [], []\n",
    "\n",
    "    for patient_id in os.listdir(base_path):\n",
    "        patient_path = os.path.join(base_path, patient_id)\n",
    "        if not os.path.isdir(patient_path):\n",
    "            continue\n",
    "\n",
    "        for label_folder in ['0', '1']:  # 0 = Non-IDC, 1 = IDC\n",
    "            label_path = os.path.join(patient_path, label_folder)\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(label_path):\n",
    "                if file.endswith('.png'):\n",
    "                    if label_folder == '0':\n",
    "                        image_paths_0.append(os.path.join(label_path, file))\n",
    "                    else:\n",
    "                        image_paths_1.append(os.path.join(label_path, file))\n",
    "\n",
    "    # Shuffle từng class\n",
    "    random.seed(seed)\n",
    "    random.shuffle(image_paths_0)\n",
    "    random.shuffle(image_paths_1)\n",
    "\n",
    "    # Tính số ảnh theo tỷ lệ 28.4% IDC : 71.6% Non-IDC\n",
    "    n_pos = int(total * 0.284)   # IDC (+)\n",
    "    n_neg = total - n_pos        # Non-IDC (−)\n",
    "\n",
    "    image_paths_1 = image_paths_1[:n_pos]\n",
    "    image_paths_0 = image_paths_0[:n_neg]\n",
    "\n",
    "    # Gộp lại và gán nhãn\n",
    "    image_paths = image_paths_0 + image_paths_1\n",
    "    labels = [0] * len(image_paths_0) + [1] * len(image_paths_1)\n",
    "\n",
    "    # Shuffle lại toàn bộ dataset\n",
    "    combined = list(zip(image_paths, labels))\n",
    "    random.shuffle(combined)\n",
    "    image_paths, labels = zip(*combined)\n",
    "\n",
    "    return list(image_paths), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:10.393457Z",
     "iopub.status.busy": "2025-08-29T21:36:10.393266Z",
     "iopub.status.idle": "2025-08-29T21:36:36.277737Z",
     "shell.execute_reply": "2025-08-29T21:36:36.276954Z",
     "shell.execute_reply.started": "2025-08-29T21:36:10.393442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 1000\n",
      "Label: 1000\n",
      "Class 0 (Benign): 716\n",
      "Class 1 (Malignant): 284\n"
     ]
    }
   ],
   "source": [
    "# 20,000 ảnh (≈ 5.7k IDC, 14.3k non-IDC)\n",
    "base_path = '/kaggle/input/breast-histopathology-images'\n",
    "image_paths, labels = load_all_images_ratio(base_path, total=1000)\n",
    "print(\"Total images loaded:\", len(image_paths))\n",
    "print(\"Label:\", len(labels))\n",
    "num_class0 = sum(1 for l in labels if l == 0)\n",
    "num_class1 = sum(1 for l in labels if l == 1)\n",
    "\n",
    "print(f\"Class 0 (Benign): {num_class0}\")\n",
    "print(f\"Class 1 (Malignant): {num_class1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:38.314473Z",
     "iopub.status.busy": "2025-08-29T21:36:38.314192Z",
     "iopub.status.idle": "2025-08-29T21:36:38.369254Z",
     "shell.execute_reply": "2025-08-29T21:36:38.368686Z",
     "shell.execute_reply.started": "2025-08-29T21:36:38.314452Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n",
      "Sample image tensor shape: torch.Size([3, 227, 227]) Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Transform theo paper (resize 227 + normalize theo CIFAR-100)\n",
    "IMG_SIZE = 227\n",
    "MEAN = (0.5071, 0.4867, 0.4408)\n",
    "STD  = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "transform_eval = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "# Dataset tuỳ chỉnh dùng list path + labels\n",
    "class IDCDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Tạo dataset và dataloader\n",
    "idc_ds = IDCDataset(image_paths, labels, transform=transform_eval)\n",
    "idc_loader = DataLoader(idc_ds, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Dataset size:\", len(idc_ds))\n",
    "img, lbl = idc_ds[0]\n",
    "print(\"Sample image tensor shape:\", img.shape, \"Label:\", lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T21:36:40.498892Z",
     "iopub.status.busy": "2025-08-29T21:36:40.498627Z",
     "iopub.status.idle": "2025-08-29T21:36:47.478436Z",
     "shell.execute_reply": "2025-08-29T21:36:47.477504Z",
     "shell.execute_reply.started": "2025-08-29T21:36:40.498871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1000, 4096) Labels: (1000,)\n"
     ]
    }
   ],
   "source": [
    "fc2_feats.clear()\n",
    "labels_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in idc_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        _ = net(inputs)                  # forward sẽ đẩy output FC2 vào fc2_feats\n",
    "        labels_all.append(labels.cpu())\n",
    "\n",
    "# Ghép lại thành ma trận N x 4096\n",
    "X = torch.cat(fc2_feats, dim=0).numpy()     # (N, 4096)\n",
    "y = torch.cat(labels_all, dim=0).numpy()    # (N,)\n",
    "print(\"Feature shape:\", X.shape, \"Labels:\", y.shape)\n",
    "\n",
    "# Bỏ hook (không cần nữa)\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:50:33.023182Z",
     "iopub.status.busy": "2025-08-29T22:50:33.022452Z",
     "iopub.status.idle": "2025-08-29T22:50:33.566408Z",
     "shell.execute_reply": "2025-08-29T22:50:33.565812Z",
     "shell.execute_reply.started": "2025-08-29T22:50:33.023140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install pynndescent\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pynndescent  # cài bằng: pip install pynndescent\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def reliefF_fast(X, y, n_neighbors=30, sample_frac=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Relief-F nhanh với Sampling + Approximate Nearest Neighbors (ANN).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray (n_samples, n_features)\n",
    "    y : ndarray (n_samples,)\n",
    "    n_neighbors : số hàng xóm gần (k)\n",
    "    sample_frac : tỷ lệ mẫu được chọn để tính (0.0 - 1.0)\n",
    "    random_state : seed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scores : ndarray (n_features,) \n",
    "        Điểm importance cho từng feature\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # --- Sampling ---\n",
    "    n_sub = int(sample_frac * n_samples)\n",
    "    idx_sub = rng.choice(n_samples, n_sub, replace=False)\n",
    "    X_sub, y_sub = X[idx_sub], y[idx_sub]\n",
    "\n",
    "    # --- ANN (Approximate Nearest Neighbor search) ---\n",
    "    index = pynndescent.NNDescent(X_sub, n_neighbors=n_neighbors+1, random_state=random_state)\n",
    "    neighbors, _ = index.query(X_sub, k=n_neighbors+1)  # +1 vì nó trả cả chính nó\n",
    "    neighbors = neighbors[:, 1:]  # bỏ chính nó đi\n",
    "\n",
    "    # --- Relief-F scoring ---\n",
    "    scores = np.zeros(n_features)\n",
    "    for i in tqdm(range(n_sub), desc=\"ReliefF (fast ANN)\"):\n",
    "        xi, yi = X_sub[i], y_sub[i]\n",
    "        for nn in neighbors[i]:\n",
    "            xj, yj = X_sub[nn], y_sub[nn]\n",
    "            diff = np.abs(xi - xj)\n",
    "            if yi == yj:  # nearest hit\n",
    "                scores -= diff / n_sub\n",
    "            else:         # nearest miss\n",
    "                scores += diff / n_sub\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:50:38.012269Z",
     "iopub.status.busy": "2025-08-29T22:50:38.011739Z",
     "iopub.status.idle": "2025-08-29T22:51:11.144951Z",
     "shell.execute_reply": "2025-08-29T22:51:11.144068Z",
     "shell.execute_reply.started": "2025-08-29T22:50:38.012244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: PCC ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PCC: 100%|██████████| 4096/4096 [00:01<00:00, 3187.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Relief-F (fast, sampling + ANN) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ReliefF (fast ANN): 100%|██████████| 200/200 [00:00<00:00, 2552.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCC branch: (1000, 1000)\n",
      "After ReliefF branch: (1000, 1000)\n",
      "Concatenated features: (1000, 2000)\n",
      "Step 3: PCA ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA fitting: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA: (1000, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tqdm import tqdm\n",
    "from skrebate import ReliefF\n",
    "\n",
    "# --- PCC score với tqdm ---\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pcc_scores_progress(X, y):\n",
    "    scores = []\n",
    "    for j in tqdm(range(X.shape[1]), desc=\"Computing PCC\"):\n",
    "        r, _ = pearsonr(X[:, j], y)\n",
    "        scores.append(abs(r))\n",
    "    return np.array(scores)\n",
    "\n",
    "print(\"Step 1: PCC ...\")\n",
    "pcc = pcc_scores_progress(X, y)  # (4096,)\n",
    "\n",
    "print(\"Step 2: Relief-F (fast, sampling + ANN) ...\")\n",
    "relief_scores = reliefF_fast(X, y, n_neighbors=30, sample_frac=0.2)\n",
    "\n",
    "# --- Chuẩn hoá về [0,1] ---\n",
    "scaler = MinMaxScaler()\n",
    "pcc_n = scaler.fit_transform(pcc.reshape(-1,1)).ravel()\n",
    "rel_n = scaler.fit_transform(relief_scores.reshape(-1,1)).ravel()\n",
    "\n",
    "# --- Chọn top-NF ---\n",
    "NF = 1000\n",
    "top_pcc = np.argsort(pcc_n)[::-1][:NF]\n",
    "top_rel = np.argsort(rel_n)[::-1][:NF]\n",
    "\n",
    "# --- Tách đặc trưng ---\n",
    "X_pcc = X[:, top_pcc]      # N x 1000\n",
    "X_rel = X[:, top_rel]      # N x 1000\n",
    "\n",
    "# --- Gộp lại ---\n",
    "X_concat = np.concatenate([X_pcc, X_rel], axis=1)  # N x 2000\n",
    "print(\"After PCC branch:\", X_pcc.shape)\n",
    "print(\"After ReliefF branch:\", X_rel.shape)\n",
    "print(\"Concatenated features:\", X_concat.shape)\n",
    "\n",
    "# --- PCA incremental với tqdm ---\n",
    "print(\"Step 3: PCA ...\")\n",
    "batch_size = 1024\n",
    "n_comp = 1000\n",
    "pca = IncrementalPCA(n_components=n_comp, batch_size=batch_size)\n",
    "\n",
    "n_samples = X_concat.shape[0]\n",
    "for start in tqdm(range(0, n_samples, batch_size), desc=\"PCA fitting\"):\n",
    "    end = min(start + batch_size, n_samples)\n",
    "    chunk = X_concat[start:end]\n",
    "\n",
    "    # Nếu batch cuối nhỏ hơn n_comp → nối với batch trước đó\n",
    "    if chunk.shape[0] < n_comp:\n",
    "        chunk = X_concat[-n_comp:]   # lấy đúng n_comp mẫu cuối\n",
    "    pca.partial_fit(chunk)\n",
    "\n",
    "X_pca = pca.transform(X_concat)\n",
    "print(\"After PCA:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:51:40.960722Z",
     "iopub.status.busy": "2025-08-29T22:51:40.960035Z",
     "iopub.status.idle": "2025-08-29T22:51:40.972335Z",
     "shell.execute_reply": "2025-08-29T22:51:40.971589Z",
     "shell.execute_reply.started": "2025-08-29T22:51:40.960700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def simple_smote(\n",
    "    X, y, *, minority_class=None,\n",
    "    target_ratio=1.0,\n",
    "    n_samples=None,\n",
    "    k=5,\n",
    "    shrink=0.7,\n",
    "    jitter_frac=1e-3,\n",
    "    random_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Nếu X, y là numpy array -> dùng SMOTE theo kNN (shrink + jitter).\n",
    "    Nếu X, y là torch.Tensor -> dùng TorchSMOTE class ở trên.\n",
    "    \"\"\"\n",
    "    # --- Nếu là torch.Tensor thì dùng TorchSMOTE ---\n",
    "    if isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor):\n",
    "        sm = TorchSMOTE(dims=X.shape[1], k=k)\n",
    "        return sm.fit_generate(X, y)\n",
    "\n",
    "    # --- Nếu là numpy thì dùng phiên bản cũ ---\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    if len(classes) != 2:\n",
    "        raise ValueError(\"Chỉ hỗ trợ nhị phân.\")\n",
    "    if minority_class is None:\n",
    "        minority_class = classes[np.argmin(counts)]\n",
    "    majority_class = classes[0] if classes[1] == minority_class else classes[1]\n",
    "\n",
    "    X_min = X[y == minority_class]\n",
    "    X_maj = X[y == majority_class]\n",
    "    n_min, n_maj = len(X_min), len(X_maj)\n",
    "\n",
    "    if n_min == 0:\n",
    "        raise ValueError(\"Không có mẫu thiểu số.\")\n",
    "    if n_samples is None:\n",
    "        target_min = int(np.ceil(target_ratio * n_maj))\n",
    "        n_to_gen = max(0, target_min - n_min)\n",
    "    else:\n",
    "        n_to_gen = int(n_samples)\n",
    "    if n_to_gen <= 0:\n",
    "        return X, y\n",
    "\n",
    "    D = X.shape[1]\n",
    "    synth = np.empty((n_to_gen, D), dtype=X.dtype)\n",
    "\n",
    "    # Jitter scale\n",
    "    std = X_min.std(axis=0, ddof=0)\n",
    "    std[std == 0] = 1.0\n",
    "    jitter_scale = jitter_frac * std\n",
    "\n",
    "    if n_min == 1:\n",
    "        noise = rng.normal(0, 1.0, size=(n_to_gen, D)).astype(X.dtype) * jitter_scale\n",
    "        synth[:] = X_min[0] + noise\n",
    "    else:\n",
    "        k_eff = max(1, min(k, n_min - 1))\n",
    "        nn = NearestNeighbors(n_neighbors=k_eff, algorithm=\"auto\").fit(X_min)\n",
    "        neigh_idx = nn.kneighbors(return_distance=False)\n",
    "\n",
    "        centroid = X_min.mean(axis=0, dtype=float)\n",
    "\n",
    "        for t in range(n_to_gen):\n",
    "            i = rng.integers(0, n_min)\n",
    "            j = int(neigh_idx[i][rng.integers(0, k_eff)])\n",
    "            lam = rng.random()\n",
    "            x_interp = X_min[i] + lam * (X_min[j] - X_min[i])\n",
    "            x_interp = centroid + shrink * (x_interp - centroid)\n",
    "            x_interp = x_interp + rng.normal(0, 1.0, size=D) * jitter_scale\n",
    "            synth[t] = x_interp.astype(X.dtype, copy=False)\n",
    "\n",
    "    X_new = np.vstack([X, synth])\n",
    "    y_new = np.hstack([y, np.full(n_to_gen, minority_class, dtype=y.dtype)])\n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:51:54.019220Z",
     "iopub.status.busy": "2025-08-29T22:51:54.018948Z",
     "iopub.status.idle": "2025-08-29T22:51:54.031144Z",
     "shell.execute_reply": "2025-08-29T22:51:54.030474Z",
     "shell.execute_reply.started": "2025-08-29T22:51:54.019202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def eval_clf(clf, X, y, n_splits=5, name=\"\", augment=False):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n",
    "    accs, sens, precs, fnrs = [], [], [], []\n",
    "\n",
    "    for tr, va in tqdm(skf.split(X, y), total=n_splits, desc=f\"CV {name}\", leave=False):\n",
    "        X_train, y_train = X[tr], y[tr]\n",
    "        X_val, y_val = X[va], y[va]\n",
    "\n",
    "        # augment bằng SMOTE nếu bật cờ augment\n",
    "        if augment:\n",
    "            X_train, y_train = simple_smote(\n",
    "                                    X_train, y_train,\n",
    "                                    target_ratio=1.0,   # cân bằng hoàn toàn\n",
    "                                    k=5,                # local neighbors\n",
    "                                    shrink=0.7,         # giữ phân phối gọn hơn (giúp LDA)\n",
    "                                    jitter_frac=1e-3,   # rất nhỏ để tránh trùng điểm\n",
    "                                    random_state=42)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred = clf.predict(X_val)\n",
    "\n",
    "        accs.append(accuracy_score(y_val, pred) * 100.0)\n",
    "        sens.append(recall_score(y_val, pred, pos_label=1) * 100.0)  # sensitivity\n",
    "        precs.append(precision_score(y_val, pred, pos_label=1, zero_division=0) * 100.0)\n",
    "\n",
    "        # Confusion matrix để tính FNR\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, pred).ravel()\n",
    "        fnrs.append(fn / (fn + tp + 1e-8) * 100.0)\n",
    "\n",
    "    return np.mean(accs), np.mean(sens), np.mean(fnrs), np.mean(precs)\n",
    "\n",
    "# --- 8 classifiers như Table trong paper ---\n",
    "classifiers = [\n",
    "    (\"Quadratic SVM\", SVC(kernel='poly', degree=2, C=1.0, gamma='scale', class_weight='balanced')),\n",
    "    (\"Cubic SVM\",     SVC(kernel='poly', degree=3, C=1.0, gamma='scale', class_weight='balanced')),\n",
    "    (\"Linear SVM\",    SVC(kernel='linear', C=1.0, class_weight='balanced')),\n",
    "    (\"Medium Gaussian SVM\", SVC(kernel='rbf', C=1.0, gamma=0.1, class_weight='balanced')),  # gamma=0.1 ~ \"medium\"\n",
    "    (\"Coarse Gaussian SVM\", SVC(kernel='rbf', C=1.0, gamma=0.01, class_weight='balanced')), # gamma nhỏ = coarse\n",
    "    (\"Ensemble Subspace Discriminant\", \n",
    "        BaggingClassifier(estimator=LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\"),\n",
    "                          n_estimators=50, max_samples=0.5, random_state=1337, n_jobs=-1)),\n",
    "    (\"Ensemble Boosted Tree\", \n",
    "        AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100, random_state=1337)),\n",
    "    (\"Fine Tree\", DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=1337))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:51:56.237161Z",
     "iopub.status.busy": "2025-08-29T22:51:56.236860Z",
     "iopub.status.idle": "2025-08-29T22:56:26.385185Z",
     "shell.execute_reply": "2025-08-29T22:56:26.384546Z",
     "shell.execute_reply.started": "2025-08-29T22:51:56.237141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classifiers:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:00<00:01,  2.70it/s]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:00<00:01,  2.67it/s]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:01<00:00,  2.68it/s]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:01<00:00,  2.78it/s]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:01<00:00,  2.73it/s]\u001b[A\n",
      "Evaluating classifiers:  12%|█▎        | 1/8 [00:01<00:12,  1.84s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:00<00:01,  2.84it/s]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:00<00:00,  3.01it/s]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:01<00:00,  2.97it/s]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:01<00:00,  2.93it/s]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:01<00:00,  2.97it/s]\u001b[A\n",
      "Evaluating classifiers:  25%|██▌       | 2/8 [00:03<00:10,  1.76s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:00<00:01,  3.14it/s]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:00<00:00,  3.20it/s]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:00<00:00,  3.26it/s]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:01<00:00,  3.28it/s]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\u001b[A\n",
      "Evaluating classifiers:  38%|███▊      | 3/8 [00:05<00:08,  1.66s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:00<00:01,  3.30it/s]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:00<00:00,  3.20it/s]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:00<00:00,  3.19it/s]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:01<00:00,  3.22it/s]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:01<00:00,  3.22it/s]\u001b[A\n",
      "Evaluating classifiers:  50%|█████     | 4/8 [00:06<00:06,  1.62s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:00<00:01,  2.76it/s]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:00<00:01,  2.90it/s]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:01<00:00,  2.79it/s]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:01<00:00,  2.86it/s]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:01<00:00,  2.84it/s]\u001b[A\n",
      "Evaluating classifiers:  62%|██████▎   | 5/8 [00:08<00:05,  1.67s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:15<01:02, 15.68s/it]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:29<00:43, 14.34s/it]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:42<00:28, 14.04s/it]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:55<00:13, 13.51s/it]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [01:09<00:00, 13.61s/it]\u001b[A\n",
      "Evaluating classifiers:  75%|███████▌  | 6/8 [01:17<00:49, 24.65s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:37<02:28, 37.18s/it]\u001b[A\n",
      "CV :  40%|████      | 2/5 [01:14<01:51, 37.33s/it]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [01:51<01:14, 37.32s/it]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [02:29<00:37, 37.35s/it]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [03:06<00:00, 37.38s/it]\u001b[A\n",
      "Evaluating classifiers:  88%|████████▊ | 7/8 [04:24<01:17, 77.65s/it]\n",
      "CV :   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  20%|██        | 1/5 [00:01<00:04,  1.19s/it]\u001b[A\n",
      "CV :  40%|████      | 2/5 [00:02<00:03,  1.10s/it]\u001b[A\n",
      "CV :  60%|██████    | 3/5 [00:03<00:02,  1.14s/it]\u001b[A\n",
      "CV :  80%|████████  | 4/5 [00:04<00:01,  1.17s/it]\u001b[A\n",
      "CV : 100%|██████████| 5/5 [00:05<00:00,  1.14s/it]\u001b[A\n",
      "Evaluating classifiers: 100%|██████████| 8/8 [04:30<00:00, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Classifier  Accuracy (%)  Sensitivity (%)   FNR (%)  Precision (%)\n",
      "                 Quadratic SVM          81.7        64.097744 35.902256      69.655665\n",
      "                     Cubic SVM          82.6        67.274436 32.725564      70.316521\n",
      "                    Linear SVM          83.1        71.842105 28.157895      69.851885\n",
      "           Medium Gaussian SVM          83.7        65.513784 34.486216      74.291506\n",
      "           Coarse Gaussian SVM          80.9        71.510025 28.489975      64.811018\n",
      "Ensemble Subspace Discriminant          77.1        26.409774 73.590226      79.984876\n",
      "         Ensemble Boosted Tree          76.6        51.434837 48.565163      60.334987\n",
      "                     Fine Tree          72.8        55.977444 44.022556      52.369274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Chạy đánh giá và gom vào bảng ---\n",
    "results = []\n",
    "for name, clf in tqdm(classifiers, desc=\"Evaluating classifiers\"):\n",
    "    mean_acc, mean_sens, mean_fnr, mean_prec = eval_clf(clf, X_pca, y, n_splits=5, augment=True)\n",
    "    results.append([name, mean_acc, mean_sens, mean_fnr, mean_prec])\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Classifier\", \"Accuracy (%)\", \"Sensitivity (%)\", \"FNR (%)\", \"Precision (%)\"])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T22:56:43.432145Z",
     "iopub.status.busy": "2025-08-29T22:56:43.431496Z",
     "iopub.status.idle": "2025-08-29T23:03:56.916899Z",
     "shell.execute_reply": "2025-08-29T23:03:56.916256Z",
     "shell.execute_reply.started": "2025-08-29T22:56:43.432119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classifiers:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:02,  4.43it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:00<00:01,  4.31it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:00<00:01,  4.22it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:00<00:01,  4.32it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:01<00:01,  4.29it/s]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:01<00:00,  4.30it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:01<00:00,  4.28it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:01<00:00,  4.32it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:02<00:00,  4.39it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:02<00:00,  4.34it/s]\u001b[A\n",
      "Evaluating classifiers:  12%|█▎        | 1/8 [00:02<00:16,  2.32s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:01,  4.79it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:00<00:01,  4.73it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:00<00:01,  4.75it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:00<00:01,  4.86it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:01<00:01,  4.77it/s]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:01<00:00,  4.85it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:01<00:00,  4.82it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:01<00:00,  4.79it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:01<00:00,  4.89it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:02<00:00,  4.83it/s]\u001b[A\n",
      "Evaluating classifiers:  25%|██▌       | 2/8 [00:04<00:13,  2.18s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:01,  4.82it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:00<00:01,  4.84it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:00<00:01,  4.81it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:00<00:01,  4.95it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:01<00:01,  4.89it/s]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:01<00:00,  4.94it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:01<00:00,  4.92it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:01<00:00,  4.93it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:02<00:00,  4.86it/s]\u001b[A\n",
      "Evaluating classifiers:  38%|███▊      | 3/8 [00:06<00:10,  2.12s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:02,  4.19it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:00<00:01,  4.23it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:00<00:01,  4.21it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:00<00:01,  4.18it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:01<00:01,  4.12it/s]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:01<00:00,  4.20it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:01<00:00,  4.15it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:01<00:00,  4.15it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:02<00:00,  4.13it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:02<00:00,  4.15it/s]\u001b[A\n",
      "Evaluating classifiers:  50%|█████     | 4/8 [00:08<00:08,  2.24s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:01,  4.73it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:00<00:01,  4.61it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:00<00:01,  4.58it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:00<00:01,  4.48it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:01<00:01,  4.57it/s]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:01<00:00,  4.63it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:01<00:00,  4.69it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:01<00:00,  4.65it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:01<00:00,  4.73it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:02<00:00,  4.66it/s]\u001b[A\n",
      "Evaluating classifiers:  62%|██████▎   | 5/8 [00:11<00:06,  2.21s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:12<01:53, 12.66s/it]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:25<01:40, 12.58s/it]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:37<01:28, 12.59s/it]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:50<01:15, 12.54s/it]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [01:02<01:02, 12.59s/it]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [01:15<00:50, 12.58s/it]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [01:28<00:37, 12.56s/it]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [01:40<00:25, 12.64s/it]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [01:53<00:12, 12.60s/it]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [02:06<00:00, 12.64s/it]\u001b[A\n",
      "Evaluating classifiers:  75%|███████▌  | 6/8 [02:17<01:28, 44.32s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:28<04:18, 28.71s/it]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:57<03:49, 28.74s/it]\u001b[A\n",
      "CV :  30%|███       | 3/10 [01:26<03:21, 28.74s/it]\u001b[A\n",
      "CV :  40%|████      | 4/10 [01:54<02:51, 28.66s/it]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [02:23<02:23, 28.66s/it]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [02:51<01:54, 28.63s/it]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [03:20<01:25, 28.65s/it]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [03:49<00:57, 28.64s/it]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [04:18<00:28, 28.66s/it]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [04:46<00:00, 28.69s/it]\u001b[A\n",
      "Evaluating classifiers:  88%|████████▊ | 7/8 [07:03<02:03, 123.57s/it]\n",
      "CV :   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "CV :  10%|█         | 1/10 [00:00<00:08,  1.08it/s]\u001b[A\n",
      "CV :  20%|██        | 2/10 [00:01<00:07,  1.04it/s]\u001b[A\n",
      "CV :  30%|███       | 3/10 [00:02<00:06,  1.05it/s]\u001b[A\n",
      "CV :  40%|████      | 4/10 [00:03<00:05,  1.03it/s]\u001b[A\n",
      "CV :  50%|█████     | 5/10 [00:04<00:05,  1.01s/it]\u001b[A\n",
      "CV :  60%|██████    | 6/10 [00:05<00:03,  1.03it/s]\u001b[A\n",
      "CV :  70%|███████   | 7/10 [00:06<00:02,  1.02it/s]\u001b[A\n",
      "CV :  80%|████████  | 8/10 [00:07<00:01,  1.09it/s]\u001b[A\n",
      "CV :  90%|█████████ | 9/10 [00:08<00:00,  1.06it/s]\u001b[A\n",
      "CV : 100%|██████████| 10/10 [00:09<00:00,  1.03it/s]\u001b[A\n",
      "Evaluating classifiers: 100%|██████████| 8/8 [07:13<00:00, 54.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Classifier  Accuracy (%)  Sensitivity (%)    FNR (%)  Precision (%)\n",
      "                 Quadratic SVM          83.4        64.088670  35.911330      74.334965\n",
      "                     Cubic SVM          82.8        63.706897  36.293103      72.349804\n",
      "                    Linear SVM          81.5        74.322660  25.677340      65.923699\n",
      "           Medium Gaussian SVM          81.2        71.810345  28.189655      65.907447\n",
      "           Coarse Gaussian SVM          79.7        73.596059  26.403941      62.296245\n",
      "Ensemble Subspace Discriminant          71.6         0.000000 100.000000       0.000000\n",
      "         Ensemble Boosted Tree          77.6        45.751232  54.248768      65.974426\n",
      "                     Fine Tree          75.5        55.603448  44.396552      57.290158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Chạy đánh giá và gom vào bảng ---\n",
    "results = []\n",
    "for name, clf in tqdm(classifiers, desc=\"Evaluating classifiers\"):\n",
    "    mean_acc, mean_sens, mean_fnr, mean_prec = eval_clf(clf, X_pca, y, n_splits=10)\n",
    "    results.append([name, mean_acc, mean_sens, mean_fnr, mean_prec])\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"Classifier\", \"Accuracy (%)\", \"Sensitivity (%)\", \"FNR (%)\", \"Precision (%)\"])\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7415,
     "sourceId": 10564,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 438474,
     "modelInstanceId": 420841,
     "sourceId": 551250,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
